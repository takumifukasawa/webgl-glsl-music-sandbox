<html>
<head>
    <meta charset="UTF-8">
</head>
<body>
<script>
// ------------------------------------------------------------------------------
// ref:
// https://qiita.com/aa_debdeb/items/5e1204987236f7b52393
// https://raku-phys.hatenablog.com/entry/2020/04/19/002400
// ------------------------------------------------------------------------------
    
    
    const vertexShaderText = `#version 300 es

uniform float uBlockOffset;
uniform float uSampleRate;

out vec2 vSound;

#define BPM 120.0

float timeToBeat(float time) {
    return time / 60. * BPM;
}

float sine(float freq, float time) {
    return sin(freq * 6.28318530718 * time);
}

// ステレオ出力のためvec2
vec2 mainSound(float time) {
    // float beat = timeToBeat(time);
    // float freq = mod(beat, 4.) >= 1. ? 440. : 880.;
    // float amp = exp(-6. * fract(beat));
    // return vec2(sine(freq, time) * amp);
   
    // 単純な減衰 
    // return vec2(sin(6.28131 * 440. * time) * exp(-3. * time));
    
    // 合計が1を超えるときがあり、きれいな波形にはならない. 音は-1~1の範囲に収まりクリップされるため
    // return vec2(sin(6.2831 * 440. * time) + sin(6.2831 * 440. * 1.5 * time));
    
    // 左右でちょっとwaveが変わるように
    // return vec2(sin(6.2831 * 440. * time) * .4 + sin(6.2831 * 440. * 1.5 * time) * .2);
    
    return vec2(sin(6.2831 * 440. * time) * fract(-2. * time));
}

void main() {
    float time = uBlockOffset + float(gl_VertexID) / uSampleRate;
    vSound = mainSound(time);
}

`;

    const fragmentShaderText = `#version 300 es
    
void main() {}
`;

    const DURATION = 180; // sec
    const SAMPLES = 65536;

    const createShader = (gl, src, type) => {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, src);
        gl.compileShader(shader);
        if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
            console.error(gl.getShaderInfoLog(shader));
            return;
        }
        return shader;
    };

    const createTransformFeedbackProgram = (gl, vertexShaderText, fragmentShaderText, varyings) => {
        const program = gl.createProgram();
        gl.attachShader(program, createShader(gl, vertexShaderText, gl.VERTEX_SHADER));
        gl.attachShader(program, createShader(gl, fragmentShaderText, gl.FRAGMENT_SHADER));
        gl.transformFeedbackVaryings(program, varyings, gl.SEPARATE_ATTRIBS);
        gl.linkProgram(program);
        if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
            console.error(gl.getProgramInfoLog(program));
            return;
        }
        return program;
    };

    const getUniformLocations = (gl, program, names) => {
        const locations = {};
        for (let i = 0; i < names.length; i++) {
            locations[names[i]] = gl.getUniformLocation(program, names[i]);
        }
        return locations;
    };

    const createVBO = (gl, data, usage = gl.STATIC_DRAW) => {
        const vbo = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, vbo);
        gl.bufferData(gl.ARRAY_BUFFER, data, usage);
        gl.bindBuffer(gl.ARRAY_BUFFER, null);
        return vbo;
    };

    const start = () => {
        const audioContext = new AudioContext();
        const audioBuffer = audioContext.createBuffer(2, audioContext.sampleRate * DURATION, audioContext.sampleRate);

        const canvas = document.createElement("canvas");
        const gl = canvas.getContext("webgl2");
        canvas.width = 512;
        canvas.height = 512;

        const program = createTransformFeedbackProgram(gl, vertexShaderText, fragmentShaderText, ["vSound"]);
        const uniformLocations = getUniformLocations(gl, program, ["uBlockOffset", "uSampleRate"]);

        const array = new Float32Array(2 * SAMPLES); // L,Rのため2倍
        const vbo = createVBO(gl, array, gl.DYNAMIC_COPY);

        const transformFeedback = gl.createTransformFeedback();

        // transform feedback 1回ごとに更新するサンプル数がSAMPLEなので、それを何回実行するかのかず
        const numBlocks = Math.ceil((audioContext.sampleRate * DURATION) / SAMPLES);
        
        const outputL = audioBuffer.getChannelData(0);
        const outputR = audioBuffer.getChannelData(1);
       
        console.log("sampleRate", audioContext.sampleRate);
        console.log("duration[sec]", DURATION);
        console.log("samples", SAMPLES);
        console.log("numBlocks", numBlocks);
        console.log("outputL len", outputL.length);
        console.log("outputR len", outputR.length);

        gl.bindTransformFeedback(gl.TRANSFORM_FEEDBACK, transformFeedback);
        
        gl.enable(gl.RASTERIZER_DISCARD);

        gl.useProgram(program);

        gl.uniform1f(uniformLocations["uSampleRate"], audioContext.sampleRate);

        for (let i = 0; i < numBlocks; i++) {
            const blockOffset = i * SAMPLES / audioContext.sampleRate;
            gl.uniform1f(uniformLocations["uBlockOffset"], blockOffset);
            gl.bindBufferBase(gl.TRANSFORM_FEEDBACK_BUFFER, 0, vbo);
            gl.beginTransformFeedback(gl.POINTS);
            gl.drawArrays(gl.POINTS, 0, SAMPLES);
            gl.endTransformFeedback();
            gl.getBufferSubData(gl.TRANSFORM_FEEDBACK_BUFFER, 0, array);

            for (let j = 0; j < SAMPLES; j++) {
                // ステレオのため
                // シェーダーの出力はvec2で、xがL, yがR
                outputL[i * SAMPLES + j] = array[j * 2];
                outputR[i * SAMPLES + j] = array[j * 2 + 1];
            }
        }
        
        gl.disable(gl.RASTERIZER_DISCARD);

        const node = audioContext.createBufferSource();
        node.connect(audioContext.destination);
        node.buffer = audioBuffer;
        node.loop = false;
        
        node.start(0);
        
        return node;
    };

    let isStarted = false;
    document.addEventListener("click", () => {
        if (!isStarted) {
            console.log("start");
            start();
        }
        isStarted = true;
    });

</script>
</body>
</html>
